{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pl.read_parquet('../input/otto-full-optimized-memory-footprint/train.parquet')\n",
    "test = pl.read_parquet('../input/otto-full-optimized-memory-footprint/test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now transform the data into a format that the `gensim` library can work with. `polars` makes the process very efficiently and quickly."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sentences_df = pl.concat([train, test]).groupby('session').agg(\n",
    "    pl.col('aid').alias('sentence')\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-05T05:05:19.777512Z",
     "iopub.execute_input": "2023-01-05T05:05:19.777770Z",
     "iopub.status.idle": "2023-01-05T05:05:28.105012Z",
     "shell.execute_reply.started": "2023-01-05T05:05:19.777746Z",
     "shell.execute_reply": "2023-01-05T05:05:28.103742Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sentences = sentences_df['sentence'].to_list()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-05T05:05:28.106139Z",
     "iopub.execute_input": "2023-01-05T05:05:28.106417Z",
     "iopub.status.idle": "2023-01-05T05:05:58.292232Z",
     "shell.execute_reply.started": "2023-01-05T05:05:28.106391Z",
     "shell.execute_reply": "2023-01-05T05:05:58.291004Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training a word2vec model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "w2vec = Word2Vec(sentences=sentences, vector_size=32, min_count=1, workers=4)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-05T05:05:58.294988Z",
     "iopub.execute_input": "2023-01-05T05:05:58.295326Z",
     "iopub.status.idle": "2023-01-05T05:31:00.271122Z",
     "shell.execute_reply.started": "2023-01-05T05:05:58.295297Z",
     "shell.execute_reply": "2023-01-05T05:31:00.269863Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 1h 12min 6s, sys: 17.9 s, total: 1h 12min 24s\nWall time: 25min 1s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the model fully train, let us use similarity between trained representations of our `aids` to create a submission.\n",
    "\n",
    "The search functionality where we look for nearest neighbors in the embedding space is built into `gensim`, but it is unfortunately super slow. Let's use `annoy` which is much faster (it performs approximate nearest neigbor search)."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "aid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\n",
    "index = AnnoyIndex(32, 'euclidean')\n",
    "\n",
    "for aid, idx in aid2idx.items():\n",
    "    index.add_item(idx, w2vec.wv.vectors[idx])\n",
    "    \n",
    "index.build(10)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-05T05:31:00.272570Z",
     "iopub.execute_input": "2023-01-05T05:31:00.272882Z",
     "iopub.status.idle": "2023-01-05T05:31:18.311518Z",
     "shell.execute_reply.started": "2023-01-05T05:31:00.272854Z",
     "shell.execute_reply": "2023-01-05T05:31:18.310350Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 44.5 s, sys: 534 ms, total: 45 s\nWall time: 18 s\n",
     "output_type": "stream"
    },
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Outputting a submission"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "sample_sub = pd.read_csv('../input/otto-recommender-system//sample_submission.csv')\n",
    "\n",
    "session_types = ['clicks', 'carts', 'orders']\n",
    "test_session_AIDs = test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n",
    "test_session_types = test.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n",
    "\n",
    "labels = []\n",
    "\n",
    "# we use the same best weight for item type as we find in Tuning Candidate ReRank Model\n",
    "# (carts are of greatest importance)\n",
    "type_weight_multipliers = {0: 0.5, 1: 9, 2: 0.5}\n",
    "for AIDs, types in zip(test_session_AIDs, test_session_types):\n",
    "    if len(AIDs) >= 20:\n",
    "        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n",
    "        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n",
    "        aids_temp=defaultdict(lambda: 0)\n",
    "        for aid,w,t in zip(AIDs,weights,types): \n",
    "            aids_temp[aid]+= w * type_weight_multipliers[t]\n",
    "            \n",
    "        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n",
    "        labels.append(sorted_aids[:20])\n",
    "    else:\n",
    "        # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n",
    "        AIDs = list(dict.fromkeys(AIDs[::-1]))\n",
    "        \n",
    "        # grab the most recent aid\n",
    "        most_recent_aid = AIDs[0]\n",
    "        \n",
    "        # look for their nearest neighbors (besides oneself)\n",
    "        nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n",
    "                        \n",
    "        labels.append((AIDs+nns)[:20])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-05T05:31:18.313093Z",
     "iopub.execute_input": "2023-01-05T05:31:18.313489Z",
     "iopub.status.idle": "2023-01-05T05:34:03.356578Z",
     "shell.execute_reply.started": "2023-01-05T05:31:18.313451Z",
     "shell.execute_reply": "2023-01-05T05:34:03.355547Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now pull it all together and write it to a file."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n",
    "\n",
    "predictions = pd.DataFrame(data={'session_type': test_session_AIDs.index, 'labels': labels_as_strings})\n",
    "\n",
    "prediction_dfs = []\n",
    "\n",
    "for st in session_types:\n",
    "    modified_predictions = predictions.copy()\n",
    "    modified_predictions.session_type = modified_predictions.session_type.astype('str') + f'_{st}'\n",
    "    prediction_dfs.append(modified_predictions)\n",
    "\n",
    "submission = pd.concat(prediction_dfs).reset_index(drop=True)\n",
    "submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-05T05:34:03.357660Z",
     "iopub.execute_input": "2023-01-05T05:34:03.357963Z",
     "iopub.status.idle": "2023-01-05T05:34:32.334626Z",
     "shell.execute_reply.started": "2023-01-05T05:34:03.357936Z",
     "shell.execute_reply": "2023-01-05T05:34:32.333291Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  }
 ]
}